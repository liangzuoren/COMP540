{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\python2env\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation data =  0.9125\n",
      "Accuracy on validation data =  0.9125\n",
      "Accuracy on validation data =  0.9125\n",
      "Accuracy on validation data =  0.94125\n",
      "Accuracy on validation data =  0.945\n",
      "Accuracy on validation data =  0.955\n",
      "Accuracy on validation data =  0.975\n",
      "Accuracy on validation data =  0.98625\n",
      "Accuracy on validation data =  0.98875\n",
      "Accuracy on validation data =  0.99\n",
      "Accuracy on validation data =  0.99125\n",
      "Accuracy on validation data =  0.99125\n",
      "Accuracy on validation data =  0.9875\n",
      "Accuracy on validation data =  0.9875\n",
      "Accuracy on validation data =  0.98625\n",
      "Accuracy on validation data =  0.98875\n",
      "Accuracy on validation data =  0.99\n",
      "Accuracy on validation data =  0.955\n",
      "Accuracy on validation data =  0.98\n",
      "Accuracy on validation data =  0.98875\n",
      "Best C =  30\n",
      "Best num iter =  10000\n",
      "Best learning rate =  0.01\n",
      "Accuracy on testing data =  0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "import utils\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "\n",
    "# load the SPAM email training dataset\n",
    "\n",
    "X,y = utils.load_mat('data/spamTrain.mat')\n",
    "yy = np.ones(y.shape)\n",
    "yy[y==0] = -1\n",
    "\n",
    "# load the SPAM email test dataset\n",
    "\n",
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "\n",
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #\n",
    "##################################################################################\n",
    "#X should be scaled to reduce computational time\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaleX = scaler.transform(X)\n",
    "\n",
    "one_array = np.ones(scaleX.shape[0])\n",
    "XX = np.concatenate([one_array[:,np.newaxis],scaleX],axis=1)\n",
    "\n",
    "#Seperating out a validation set for optimization of C, num_iters, and learning rates\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaleX = scaler.transform(X)\n",
    "\n",
    "one_array = np.ones(scaleX.shape[0])\n",
    "XX = np.concatenate([one_array[:,np.newaxis],scaleX],axis=1)\n",
    "\n",
    "#Seperating out a validation set for optimization of C, num_iters, and learning rates\n",
    "training_set_size = int(XX.shape[0]*0.8)\n",
    "random_sampling = np.random.randint(XX.shape[0],size = XX.shape[0])\n",
    "scrambled_data = XX[random_sampling,:]\n",
    "scrambled_labels = yy[random_sampling]\n",
    "XX_train = scrambled_data[:training_set_size,:]\n",
    "XX_val = scrambled_data[training_set_size:,:]\n",
    "yy_train = scrambled_labels[:training_set_size]\n",
    "yy_val = scrambled_labels[training_set_size:]\n",
    "\n",
    "Cvals = [0.01,0.03,0.1,0.3,1,3,10,30]\n",
    "#Testing out different C_vals\n",
    "best_accuracy = 0\n",
    "best_C = 0\n",
    "for C in Cvals:\n",
    "    svm = LinearSVM_twoclass()\n",
    "    svm.theta = np.zeros((XX_train.shape[1],))\n",
    "    svm.train(XX_train,yy_train,learning_rate=1e-4,reg=C,num_iters=1000,verbose=False,batch_size=XX_train.shape[0])\n",
    "\n",
    "    y_pred = svm.predict(XX_val)\n",
    "    accuracy = metrics.accuracy_score(yy_val,y_pred)\n",
    "    print \"Accuracy on validation data = \", metrics.accuracy_score(yy_val,y_pred)\n",
    "    if accuracy>best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_C = C\n",
    "        best_svm = svm\n",
    "\n",
    "#Testing different learning rates and number of iterations to find optimal one\n",
    "learning_rates = [1e-2,1e-3,1e-4,1e-5]\n",
    "num_iters = [1000,5000,10000]\n",
    "best_accuracy = 0\n",
    "best_learning_rate = 0\n",
    "best_num_iter = 0\n",
    "for learning_rate in learning_rates:\n",
    "    for num_iter in num_iters:\n",
    "        svm.theta = np.zeros((XX_train.shape[1],))\n",
    "        svm.train(XX_train,yy_train,learning_rate,best_C,num_iter,verbose=False,batch_size=XX_train.shape[0])\n",
    "\n",
    "        y_pred = svm.predict(XX_val)\n",
    "        accuracy = metrics.accuracy_score(yy_val,y_pred)\n",
    "        print \"Accuracy on validation data = \", metrics.accuracy_score(yy_val,y_pred)\n",
    "        if accuracy>best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_num_iter = num_iter\n",
    "            best_learning_rate = learning_rate\n",
    "\n",
    "print \"Best C = \", best_C\n",
    "print \"Best num iter = \", best_num_iter\n",
    "print \"Best learning rate = \", best_learning_rate\n",
    "\n",
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's performance                       #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test==0] = -1\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_test)\n",
    "scaleX = scaler.transform(X_test)\n",
    "\n",
    "one_array = np.ones(scaleX.shape[0])\n",
    "XX_test = np.concatenate([one_array[:,np.newaxis],scaleX],axis=1)\n",
    "\n",
    "y_pred = best_svm.predict(XX_test)\n",
    "print \"Accuracy on testing data = \", metrics.accuracy_score(yy_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing data =  0.977\n",
      "Accuracy on training data =  0.98425\n",
      "[0.13298095 0.10820128 0.08204523 0.07765171 0.07716495 0.07148931\n",
      " 0.07063565 0.06857259 0.06591054 0.06246492 0.06095593 0.05679279\n",
      " 0.05622434 0.04853656 0.04790067]\n",
      "clearli\n",
      "remot\n",
      "base\n",
      "otherwis\n",
      "herba\n",
      "young\n",
      "gt\n",
      "natur\n",
      "player\n",
      "franc\n",
      "believ\n",
      "york\n",
      "off\n",
      "creativ\n",
      "fb\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "y_pred = best_svm.predict(XX_test)\n",
    "print \"Accuracy on testing data = \", metrics.accuracy_score(yy_test,y_pred)\n",
    "\n",
    "y_pred = best_svm.predict(XX)\n",
    "print \"Accuracy on training data = \", metrics.accuracy_score(yy,y_pred)\n",
    "\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "\n",
    "non_one_thetas = best_svm.theta[1:]\n",
    "sorted_thetas = non_one_thetas.argsort()[-15:][::-1]\n",
    "for theta in sorted_thetas:\n",
    "    print words[theta]\n",
    "#Printed below are the words most associated with spam    \n",
    "    \n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #\n",
    "##################################################################################\n",
    "#It seems that the linear models do classify the test data very well with a testing data accuracy of 0.977 and\n",
    "#training data accuracy of 0.98425\n",
    "\n",
    "#We will test a simple Gaussian Kernel to see if we the performance improves\n",
    "#Attempting a Gaussian Kernel\n",
    "sigma = 1\n",
    "\n",
    "#Utilizing the scaled value of X to avoid overflow in the Gaussian Kernel\n",
    "K = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in XX_train for x2 in XX_train]).reshape(XX_train.shape[0],XX_train.shape[0])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "svm.theta = np.zeros((KK.shape[1],))\n",
    "\n",
    "svm.train(KK,yy_train,learning_rate=best_learning_rate,reg=best_C,num_iters=best_num_iters,verbose=True,batch_size=KK.shape[0])\n",
    "\n",
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's performance                       #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "\n",
    "Ktest = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in XX_test for x2 in XX_train]).reshape(XX_test.shape[0],XX_train.shape[0])\n",
    "scaler = preprocessing.StandardScaler().fit(Kval)\n",
    "scaleK = scaler.transform(Kval)\n",
    "\n",
    "KKtest = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "\n",
    "y_pred = svm.predict(KKtest)\n",
    "accuracy = metrics.accuracy_score(yy_test,y_pred)\n",
    "print \"Accuracy on testing data = \", metrics.accuracy_score(yy_test,y_pred)\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "\n",
    "non_one_thetas = best_svm.theta[1:]\n",
    "sorted_thetas = non_one_thetas.argsort()[-15:][::-1]\n",
    "for theta in sorted_thetas:\n",
    "    print words[theta]\n",
    "\n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
